{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfmTsxwPouoC"
      },
      "source": [
        "# EDA(Exploratory Data Analysis)\n",
        "![](./main.jpg)\n",
        "- 우리는 왜 지금까지 `pandas`로 데이터를 가공하고, 그런 데이터를 `matplotlib`과 `seaborn`으로 시각화 했을까요? \n",
        "- 사실 이러한 과정이 모두 EDA를 위한 것이라고 해도 과언이 아닙니다. EDA는 우리가 불러온 데이터를 전처리하고 분석을 하면서 그 안에서 유의미한 결과를 추출하는 과정을 말합니다. \n",
        "- EDA만 잘 해도 우리가 힘들게 머신러닝이나 딥러닝을 돌리지 않아도 충분히 데이터 자체에서 유의미한 인사이트를 추출할 수 있죠. 뿐만 아니라 데이터가 어떻게 생겼는지 EDA를 통해 파악함으로써, 모델을 어떻게 설계해야 할지, 전처리 전략은 어떻게 가져가야할 지 구상할 수 있습니다. \n",
        "- 여기까지만 들으셔도 왜 EDA를 꼭 해야하는지 감이 잡히시죠? 아직 감이 안 잡히시는 분들을 위해 예시 하나만 들어드릴게요.\n",
        "```   \n",
        "    - A와 B는 같은 데이터를 가지고 모델링을 해서 누가누가 더 좋은 성능을 내는지 내기를 했습니다.\n",
        "    - A는 EDA부터 차근찬근 진행하였고, B는 EDA를 하지 않고 기본적인 전처리만 하고 모델에 넣었습니다.\n",
        "    - 그러나 성능은 A의 것이 훨씬 좋게 나왔습니다. 알고보니 Target Data에 몇몇 Noise가 껴있었고, A는 이를 EDA과정에서 발견하고 가공 후 모델링을 진행했던 것입니다.\n",
        "    - A는 그 과정에서 Target Data의 Imbalance도 발견했습니다. A는 이를 보고 loss에 weight를 다르게 주는 방법을 적용하여 B와 같은 모델을 사용했지만 모델이 데이터 불균형에 더 강건하게 학습하도록 만들었습니다.\n",
        "    - A는 B와의 내기에서 이겨 행복하게 살았답니다~\n",
        "```\n",
        "- 위에서 말하는 Target Data에 노이즈가 어쩌구~ Imbalance가 어쩌구~ Loss의 Weight를 어쩌구~ 는 무시해도 좋습니다. 다만 우리가 꼭 가져가야할 교훈은 EDA를 어떻게 하냐에 따라 모델링의 전략이 달라지고, 데이터 가공이라는 요소는, 우리 모델의 성능을 가장 크게 바꿀 수 있는 변수 중에 하나입니다. \n",
        "\n",
        "- 머신러닝엔 이런 말이 있습니다. **Garbage in, Garbage out**. 그만큼 우리가 먹이로 주는 데이터의 상태가 모델의 성능을 결정하는 데에 있어 매우 중요한 요소라는 것이죠. 잘 와닿지 않으신다구요? 앞으로 모델링을 하시다보면 왜 이런 말이 나왔는지 아시게 될 거예요 ㅎㅎ\n",
        "\n",
        "- 그럼 이제부터 데이터 분석 및 모델링의 가장 기본 과정인 EDA를 실습해보겠습니다. 주어진 데이터를 여러분이 지금까지 배우면서 알아낸 기술로 EDA를 진행해볼 겁니다. 시작해보시죠~!\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "**알림**\n",
        "- 본 컨텐츠는 강의 형식이 아닌, 스스로 공부하시는 분들을 위한 일종의 문제집 입니다.\n",
        "- 데이터라는 큰 바다에서 여러분이 쓸 데 없는 시간 낭비 없이 바로바로 핵심을 배우실 수 있도록 커리큘럼을 짜봤습니다.\n",
        "- 이 컨텐츠의 문제들만 해결한다고 실력이 오르지 않습니다. 본 컨텐츠의 목적은 문제를 해결하는 과정에서 발생하는 고민과 질문을 통한 실력 향상입니다. \n",
        "- 문제에서 절대 떠먹여주지 않습니다. 물고기를 잡아주는 것이 아닌, 물고기를 잡는 방법을 여러분이 이 컨텐츠를 통해 알아가셨으면 합니다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VmMoB_Lu10iD"
      },
      "source": [
        "# 1. EDA 기본\n",
        "- 우리는 이제부터 공공데이터를 활용해서 그 안에서 우리가 어떤 정보를 얻어낼 수 있는지 살펴보고자 합니다. 우리가 지금까지 연습했던 `pandas`와 `시각화 도구`를 어떻게 활용할 수 있을지 한번 알아봅시다!"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "NyMhQGAnyrqS"
      },
      "source": [
        "### 1-1 데이터 읽어오기\n",
        "[이곳](https://drive.google.com/drive/folders/1xpvgkrmfrE8OZ5pQAbc5g9_3czaOvOTn)에서 데이터를 다운받아 읽어오는 코드를 작성해주세요.  \n",
        "\n",
        "- 파일을 받아보시면, CSV 파일이 여러개가 존재합니다. 이 파일들을 1개의 Data Frame으로 만드는 것이 목표입니다.\n",
        "- 단, 만들어진 최종 Data Frame의 index에 주의해주세요! (단순 concat을 시키면 index가... 어떻게 되었죠?)\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "from glob import glob\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore') # 경고 메세지를 출력 안 하는 코드입니다.\n",
        "\n",
        "base_dir = \"HERE YOUR DIR!\" # 알집 풀어주신 곳 폴더를 넣으시면 됩니다.\n",
        "file_lst = glob(base_dir + \"*.csv\")\n",
        "\n",
        "\n",
        "> HERE YOUR CODE!!\n",
        "\n",
        "\n",
        "concat_df\n",
        "\n",
        "```\n",
        "  \n",
        "검색 힌트 : pandas concat, pandas read csv, \n",
        "n, pandas concat, pandas reset index\n",
        "  \n",
        "_예시_  \n",
        "![](./1-1answer.png)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xe_nftDjlU26"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from glob import glob\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "base_dir = '/content/drive/MyDrive/C:\\Users\\user\\OneDrive\\바탕 화면\\03_EDA'\n",
        "file_lst = glob(base_dir + \"*.csv\")\n",
        "\n",
        "# Create an empty list to store dataframes\n",
        "dfs = []\n",
        "\n",
        "# Loop through each CSV file and read it into a dataframe\n",
        "for file in file_lst:\n",
        "    df = pd.read_csv(file)\n",
        "    dfs.append(df)\n",
        "\n",
        "# Concatenate all the dataframes in the list vertically\n",
        "df_combined = pd.concat(dfs, axis=0, ignore_index=True)\n",
        "\n",
        "# Print the first 5 rows of the combined dataframe\n",
        "print(df_combined.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACMPRCgi1DNr"
      },
      "source": [
        "### 1-2 카페만 조회하기\n",
        "1-1에서 만든 Data Frame에서 `상권업종중분류명`이 `커피점/카페`인 데이터들만 조회하는 코드를 작성해주세요.\n",
        "\n",
        "<br>\n",
        "\n",
        "```python\n",
        "\"\"\"~ Codes from 1-1\"\"\"\n",
        "\n",
        "concat_df = > HERE YOUR CODE!!\n",
        "\n",
        "concat_df = concat_df.reset_index(drop=True) # inplace 쓰셔도 됩니다~\n",
        "concat_df\n",
        "```\n",
        "\n",
        "<br> \n",
        "\n",
        "검색 힌트 : pandas indexing, pandas fency indexing\n",
        "  \n",
        "_예시_  \n",
        "![](./1-2answer.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "amOBB7q21Dsu"
      },
      "outputs": [],
      "source": [
        "coffee_df = df_combined[df_combined['상권업종중분류명'] == '커피점/카페']\n",
        "coffee_df = coffee_df.reset_index(drop=True) # reset index to start from 0\n",
        "print(coffee_df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-zChnS71EIZ"
      },
      "source": [
        "### 1-3 특정한 카페만 조회하기\n",
        "1-2에서 만든 Data Frame에서 우리는 전국 카페의 정보를 얻어냈습니다. 이제 이중에서 `마포구`에 위치한 `스타벅스`의 데이터만 조회하는 코드를 작성해주세요.  \n",
        "  \n",
        "- 가만 데이터를 살펴보니 상호명이 순수하게 `스타벅스`인 곳만 있는 게 아니라 `스타벅스 ~~점`처럼 지점 이름이 포함된 곳도 있습니다. 이런 스타벅스도 조회하려면 어떻게 해야할까요?\n",
        "  \n",
        "<br>\n",
        "\n",
        "```python\n",
        "\"\"\"~ Codes from 1-2\"\"\"\n",
        "\n",
        "concat_df = > HERE YOUR CODE!!\n",
        "\n",
        "concat_df = concat_df.reset_index(drop=True) # inplace 쓰셔도 됩니다~\n",
        "concat_df\n",
        "```\n",
        "\n",
        "<br> \n",
        "\n",
        "검색 힌트 : pandas indexing, pandas fency indexing, python str contains, python str startswith\n",
        "  \n",
        "<br>\n",
        "\n",
        "_예시_  \n",
        "![](./1-3answer.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "starbucks_df = coffee_df[coffee_df['상호명'].str.contains('스타벅스') & (coffee_df['시군구명'] == '마포구')]\n",
        "starbucks_df = starbucks_df.reset_index(drop=True)\n",
        "print(starbucks_df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWJishLq1Enh"
      },
      "source": [
        "### 1-4 프렌차이즈 카페 조회해보기\n",
        "마포구에 있는 스타벅스의 데이터를 모두 조회해봤습니다. 이제 스타벅스 뿐만 아니라, `투썸`, `이디야`, `할리스`, `메가커피`, `커피빈`에 대한 정보도 한번 가져와봅시다!  \n",
        "위에서 명시된 프렌차이즈 카페(스타벅스 포함)들의 마포구 내의 점포 수와, 입점 비율 정보가 담겨있는 Data Frame을 생성하는 코드를 작성해주세요.  \n",
        "  \n",
        "- 각 카페의 상호명은 아래 `cafe_lst`변수의 이름이 포함되어 있는 것을 기준으로 합니다. \n",
        "    - ex) 스타벅스홍대T동점 -> O , Starbucks홍대T동점 -> X\n",
        "  \n",
        "<br>\n",
        "\n",
        "```python\n",
        "\"\"\"from 1-1 code's Data Frame!!\"\"\"\n",
        "\n",
        "mapo_cafes = ori_df[(ori_df['시군구명'] == \"마포구\") & (ori_df['상권업종중분류명'] == \"커피점/카페\")].reset_index(drop=True)\n",
        "\n",
        "tot_cafes = len(mapo_cafes)\n",
        "\n",
        "cafe_lst = ['스타벅스', \"투썸플레이스\", \"이디야\", \"할리스\", \"메가커피\", \"커피빈\"]\n",
        "nums = []\n",
        "ratio = []\n",
        "\n",
        "> HERE YOUR CODE!!\n",
        "\n",
        "mapo_cafe_info\n",
        "```\n",
        "\n",
        "<br> \n",
        "\n",
        "검색 힌트 : pandas indexing, pandas fency indexing, python str contains, python str startswith, python f-string, python string format\n",
        "  \n",
        "<br>\n",
        "\n",
        "_예시_  \n",
        "![](./1-4answer.png)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for cafe in cafe_lst:\n",
        "    num = len(mapo_cafes[mapo_cafes['상호명'].str.contains(cafe)])\n",
        "    nums.append(num)\n",
        "    ratio.append(num / tot_cafes)\n",
        "\n",
        "df_franchise = pd.DataFrame({'프렌차이즈': cafe_lst, '마포구 내 점포 수': nums, '입점 비율': ratio})\n",
        "print(df_franchise)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HF3P9FTX3A8h"
      },
      "source": [
        "### 1-5 시각화하기\n",
        "데이터프레임으로만 나타내면 읽을 수는 있겠지만, 너무 오래 보면 눈이 피곤할 것같습니다. 1-4에서 얻어낸 입점 비율을 간단하게 시각화하는 코드를 작성해주세요.  \n",
        "단, 입점비율이 가장 높은 프렌차이즈는 강조해서 표시해주세요.  \n",
        "\n",
        "- 본 문제는 여러분이 배웠던 시각화 스킬 중에 가장 적절하다고 판단되는 방법을 사용해주세요. `bar plot`도 좋고, `pie chart`도 좋습니다 :)  \n",
        "\n",
        "```python\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "> HERE YOUR FREE CODE!! \n",
        "\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "<br>\n",
        "\n",
        "검색 힌트 : matplotlib bar plot, matplotlib pie chart, matplotlib color"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k5Z_kn8O3Bao"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XtU9p7lI4hEW"
      },
      "source": [
        "### 1-7 인사이트\n",
        "우리는 간단한 데이터로, 간단한 EDA를 진행해봤습니다. 여러분은 어떤 인사이트를 얻어냈나요? 우리는 마포구의 카페 정보만 알아봤지만, 여러분이 살고 계시는 지역의 카페, 카페가 아니더라도 음식점의 입점 비율을 확인해보든가도 가능할 것같습니다.\n",
        "\n",
        "여러분이 알고 싶은 정보를 뽑아내기 위해, 지역이나 업종을 달리해서 알아내보세요! 이런 걸 시도해볼 수 있을 거 같아요 :)\n",
        "- 서울시에서 스타벅스가 가장 많은 구는 어디인가요?\n",
        "- 서울시를 제외한 지역에서 가장 인기가 많은 프렌차이즈는 어디일까요?\n",
        "- 본인이 사는 혹은 고향의 상권 정보도 위의 과정으로 한번 알아볼까요?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TQBnwXRA4hj3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2h1EJ19l4AJi"
      },
      "source": [
        "# 2. 마케팅 데이터 EDA 해보기\n",
        "- 1부에선 데이터 자체를 뚝딱뚝딱 거리면서 그 안에서 우리가 얻을 수 있는 정보가 무엇이 있을지 생각해봤습니다.\n",
        "- 그럼 이제, 우리가 5주차부터 하게될 `Machine Learning`과 `Deep Learning` 모델링을 하기 전에, 데이터로부터 어떤 전략을 짤 수 있는지 한번 알아보겠습니다.\n",
        "- **❗❗물론❗❗** 우리는 EDA 단계만 하는 것이기 때문에, 모델링을 위한 데이터 전처리는 5주차부터 알아볼 거예요 :) 지금은 그냥 가볍게~ 어떤 정보들을 확인해야 하는지만 알아보는 과정으로 이해해주세요 ㅎㅎ \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhnPDGRC4YP_"
      },
      "source": [
        "### 2-1 데이터 읽어오기\n",
        "[이곳](https://www.kaggle.com/datasets/vetrirah/customer)에서 데이터를 받아 데이터를 읽어오세요. Train Data만 읽어오시면 됩니다.\n",
        "\n",
        "```python \n",
        "import pandas as pd\n",
        "\n",
        "> HERE YOUR CODE!!\n",
        "\n",
        "df\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j2bePsmq4YlH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-LHcyIZ4Y9a"
      },
      "source": [
        "### 2-2 결측치 확인하기\n",
        "❗잠깐❗  \n",
        "`!pip insatll missingno`로 `missingno`를 설치해주세요!  \n",
        "  \n",
        "불러온 데이터의 Column 별 결측치(Missing Value) 정보를 출력하는 코드와  \n",
        "`missingno`를 활용하여 결측치를 시각화해보세요.\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "import missingno as msno\n",
        "\n",
        "> Codes that print Missing Values\n",
        "\n",
        "> Codes that visualize Missing Values with `missingno`\n",
        "```\n",
        "  \n",
        "검색 힌트 : python missingno, missingno matrix, missingno bar, pandas isnull()\n",
        "\n",
        "_예시_  \n",
        "![](./2-2answer.png)  \n",
        "![](./2-2answer2.png)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w-e0GI_c4ZTH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2-3 Distribution Check\n",
        "2-1에서 불러온 Data Frame의 Column별 분포를 확인할 수 있는 코드를 작성해주세요.  \n",
        "- 단, _예시_ 처럼 한 눈에 전체 Column의 분포를 확인할 수 있게 시각화해보세요. \n",
        "- 데이터의 분포를 확인하는 것만으로도, 이상치를 어떻게 대처해야할지 플랜을 세울 수 있습니다 :)  \n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "import matploblib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "> HERE YOUR CODE!!\n",
        "```\n",
        "\n",
        "<br>\n",
        "\n",
        "검색 힌트 : seaborn countplot, seaborn distplot seaborn kdeplot, matplotlib for문 여러개 그리기, plt.subplots_adjust, plt.style(\"ggplot\")\n",
        "\n",
        "<br>\n",
        "\n",
        "_예시_  \n",
        "![](./2-3answer.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2-4 Correlation Check\n",
        "변수간 상관관계가 무엇일까요? 상관관계가 무엇인지 여러분만의 답을 적어주세요.  \n",
        "추가적으로, 2-1에서 불러온 Data Frame의 Column 간 `상관관계`를 시각화하여 확인할 수 있는 코드를 작성하고, 이를 해석해보세요.  \n",
        "\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "import matploblib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "> HERE YOUR CODE!!\n",
        "```\n",
        "\n",
        "<br>\n",
        "\n",
        "검색 힌트 : seaborn heatmap, seaborn heatmap parameters, seaborn mask\n",
        "\n",
        "<br>\n",
        "\n",
        "_예시_  \n",
        "![](./2-4answer.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 3주차 종료\n",
        "- EDA 문제를 모두 해결하신 여러분 고생많으셨습니다. EDA는 초입에서 말씀 드린 것처럼, 그 자체로 데이터를 분석할 수 있는 강력한 스킬이 될 수 있고, 추후에 우리가 문제를 풀어볼 모델링의 방향성을 잡는 데에도 쓰일 수 있답니다 :)   \n",
        "- 여러분들이 이번 컨텐츠를 해결하시면서 Raw Data 가공법, 그 안에서 쓸만한 데이터를 추출할 수 있는 방법, 그리고 그것을 다른 사람들에게 전달하는 스킬을 얻으셨으면 합니다.\n",
        "- 질문은 언제든 `discord 채널`을 활용해주세요! \n",
        "\n",
        "# 숙제\n",
        "- 역시 숙제가 있습니다.\n",
        "- 여러분은 오늘 공공데이터를 여러분 나름의 방식으로 EDA해보셨습니다. 오늘 EDA하신 내용과 그 과정을 여러분들의 블로그에 정리해서 올려주세요. \n",
        "- 데이터의 자료형이나 특성이 이러해서 어떠한 방법을 사용했는지, 그런 특성상 어떤 시각화 기법을 이용하는 것이 좋겠다고 판단했는지를 적다보면, 단순히 글을 올린다는 의미보다 추상적이었던 지식과 경험을 여러분들만의 언어로 구체화 하고 있다는 것을 느끼실 수 있을 겁니다.  \n",
        "- 이 숙제 역시 정답은 없습니다! 여러분만의 EDA를 블로그에 업로드 해주세요!\n",
        "  \n",
        "\n",
        "## 심화 숙제\n",
        "🚀 다른 공공데이터를 활용하시면 여러분 실력 향상에 더 좋습니다 :) [이곳](https://www.data.go.kr/)에서 여러분들 입맛에 맞는 데이터를 선정하신 뒤, EDA를 하시는 것도 큰 도움이 될 거예요!\n",
        "- 공공데이터를 선정하신 후에 여러분들만의 EDA를 진행해보세요! 그리고 그 결과를 블로그에 정리해보세요! 이 과정을 거치시면 여러분들의 실력이 굉----장히 올라있을 겁니다!\n",
        "- 심화 숙제를 클리어하신 분들은 `@자기혁명왕`에게 블로그 링크와 함께 DM을 보내주세요! (나중에 우수 스터디원 선정에 반영될 수도~?)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.8.5 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "vscode": {
      "interpreter": {
        "hash": "768016c7aa7eaabec9016402577e4cabb606e332b3187c608d53990dc1c3c37f"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
